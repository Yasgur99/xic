# Programming Assignment 2 - Implementing Semantic Analysis

## Collaborators
* Michael Maitland, mtm68
* Scott Bass, sb2383
* Michael Tobin, mat292

## Running the Program

**Main class:** `mtm68.Main.java`

Run `xic-build` (this requires having Maven installed). Then `xic [options...] <source-files>` becomes available.

## Summary

We found that accounting for all the different possible legal syntax to be challenging. There were a lot of semantically illegal but syntactically legal structures that we would think of and have to add to our parser's abilities (for example true[3]). Design-wise, we had to decide how we were going to both create our grammar and also structure our AST. The AST design followed very closely once we had our grammar since most nonterminals simply corresponded to a node type in our AST. As for our grammar, we had to sit down and draw up a class hierarchy that is inspired by the partial S-Expression grammar given in the writeup. A lot of the issues we ran into while programming were unfortunately integration issues with jcup and jflex as well as different build errors due to versioning issues. These are always frustrating to solve, but once we got them out of the way, we were able to go into a comfortable workflow. Bugs of course came up here and there but were no big deal to resolve.

## Specification

Again, in this assignment we did not feel we had to make many choices about the spec's ambiguity other than a few small things.

One choice we made was how to handle an empty file. We decided, that since it was left ambiguous, that the behavior should be undefined. Currently our parser throws a syntax error for an unexpected EOF token. We believe this a reasonable choice as any user should not expect an empty file to do anything.

Another choice was dealing with two different forms of array initializers. We decided that we will not allow initializing an array with specificed ranges with an array initializer (i.e. a:int[3] = {1, 2, 3}). We decided that the language used in the specification implied these two forms are not compatible. Also, this is not a feature in many major languages like Java as it simply would not serve much of a useful purpose.

We also made the choice that function declarations cannot be separated by semicolons.

## Design and Implementation 
One of the important parts of this assignment was creating a grammar to capture the xi language and put it into our cup spec. Luckily enough, after designing our grammar by hand, there turned out to be no original shift-reduce or reduce-reduce conflicts. We did, however, run into a few later down the road as we tweaked our grammar. One example was when we were building our possible array initializer list (which allows for a trailing comma). We initially used a right recursive approach but then there was an error where, if the list was ended with a trailing comma, the parser didn't know to shift and look for another expression or reduce and end the list. Upon some research and a few different remodeling attempts, we found that changing the list to build left-recursively fixed the problem with no real downside.

### Architecture ###
The key classes and packages we created or updated for this assignment are the following...
 
- mtm68.Main.java
    - Our Main functions very similarly to the previous assignment. We simply added parsing to the program pipeline as well as command line options for outputting a .parsed file and specifying a source path.
 				  
- mtm68.parser.Parser.java & mtm68.parser.xi.cup
    - Parser is the autogenerated class created by Cup. We have modified our lexer.flex in order to allow for seamless integration with cup. This required making our Tokens extend ComplexSymbol which allowed them to be directly passed directly from the lexer to the parser during parsing. We created a TokenFactory class which takes in info from the Lexer (such as token type and location) and constructs into the format that the parser expects. 
    - Our cup is organized quite similarly to the S-Expression spec in the assignment writeup. Our main non-terminals are Program, Interface, Expr, Decl, Use, and Statement. We then have a number of other nonterminals that deal with list construction of different types of nonterminals (i.e. use_star). These types of nonterminals are named based on a regex-like construction (i.e. use_star = use*). Each match returns a result that corresponds to the objects AST node (which is discussed in general further down).
    
- mtm68.ast.nodes & mtm68.ast.nodes.binary & mtm68.ast.nodes.stmts & mtm68.ast.types
    - In these packages, we have created a sizable number of classes that correspond to all of the different possible AST nodes. These nodes correspond directly to the nonterminals in our cup file (i.e. exp:e1 ADD exp:e2 --> Add(e1, e2)). We have established a hierarchy in order to reduce boilerplate code and maintain an organized structure. A great example of this is how all binary operations such as Add and Mult extend our BinExpr class. This came in handy when implementing pretty-printing and will also be useful in the future when creating typechecking and other passes through the AST. This structure will allow for less code repetition.
    
- mtm68.lexer.FileTypeLexer.java
    - This is a wrapper class for the Lexer generated by flex. This wrapper class is what is passed to the parser. It's main purpose is to help the parser tell whether the file being parsed is a program or an interface file. It does this by passing the parser either a XI or IXI token upon the parsers first call to the lexer's next_token method and then simply giving the rest of the tokens normally. We felt this was important to address as the parser can tell whether or not it is parsing a program or interface on its own but it is important that what it is parsing matches the filetype. For example, without these tokens, the parser would parse a perfectly normal program stored in an interface file and not complain (which is bad).
    
- mtm68.parser.ParseResult.java
    - We decided it would be useful to create a class that wraps the Parser result whether it be a valid AST or a list of syntax errors. This allows us finer control over the handling of errors and how they are outputted to the user. After parsing, we can check if there was a successful AST parsed or, if need be, access the information about the errors thrown.
    
### Code Design ###
- For this assignment, we had to implement was a tree traversal for pretty printing. Essentially each node of our AST have to implement a prettyPrint(SExpPrinter p) method. This allows us to call prettyPrint on the root node which then propagates throughout the rest of the tree in order to print the S-Expression model.
- The main data structure we used was a simple tree to maintain our AST as it was built by the parser. Each node is slightly different based on its subcomponents. For example, an ADD node must maintain two child expressions that are operands. Another example is that a FunctionDefn node has a FunctionDecl child and a Block child which represent its declaration and function body.
- Early on, we decided not to use a Visitor structure for our pretty print setup (as we were unsure how/if it would fit later parts of the project well). Of course after our class lecture on the visitor structure we were a little disappointed that we hadn't used it. Perhaps we will incorporate the structure later regardless.

### Programming ###
- Once again, we ran into a lot of interesting errors for our build and also incorporating Jflex and cup. These errors were often tough to chase down due to unhelpful error messages but fortunately we were able to solve them after a bit of debugging. One example of such an error was not constructing an EOF token of our own. The one passed to the parser by default was not of a format that matched our other tokens which caused mysterious errors.
- The following is the team coding/responsibility breakdown for this assignment...
    - **Tobin:** 
       - command line options
       - jcup spec
       - pretty printing (and pp tests)
    - **Maitland:**
        - fixing lexer errors
        - new lexer tests
        - parser tests 
    - **Bass:**
        - jcup spec
        - AST nodes 
        - parser tests
 - We, of course, reused our lexer code from the previous assignment. There were a number of errors that we had to fix (mostly involving strings) and also a few slight changes to handle cup integration. This is discussed in the architecture section.
 
## Testing
First, we added a number of tests to our Lexer test suite to address the errors we ran into during PA1 grading. We added a test case to corresepond to each of the failing test cases from the grading. This allowed us to systematically address each of the errors and ensure that they were all fixed. A lot of the errors had to deal with matching on escaped characters in character literals. We fixed these by adding cases for the lexer to match on for each of them.

In order to test our code, we utilized a suite of JUnit tests. These tests ensure that we correctly parse and construct an AST for a given program. This time around, there were a lot more different edge cases and structures in general to test. This required really considering different and unusual syntactically legal programs which was a challenge in itself. Our tests are structured to create dummy programs which encased the structures we wanted to test. Then we are able to assert different things about the AST returned.

We also created a test suite for pretty printing to check that our structure was being outputted correctly. These test cases incorporated a few of the release tests as well as some of our own design. In order to run these tests, we create a Lexer that wraps around a StringReader that is fed the desired program string. We then give that Lexer to a Parser which outputs its pretty printing to a StringWriter. Then we are able to compare the output of the StringWriter with our expected result string. For errors, we pull the first error off of the list of errors in the ParseResult and compare the message to the intended error message. With these tests, we were able to catch some structural bugs with parentheses in different S-Expression prints. We also caught a parse error that had to do with array index precedence. 

## Work plan

We began by splitting up the work such that Maitland would address flaws in our lexer as well as jcup integration while Bass and Tobin worked on the grammar and cup spec. This allowed us to accomplish two big tasks in parallel without holding each other back. Then, we caught each other up to speed and carried out testing together to make sure everyone understood the code and that it was correct.

This plan allowed for us to work in parallel which increased our efficiency. We also partitioned the work such that each component was well-isolated which meant that there wasn't much need for code redesign. We had also discussed what was expected of each component prior to implementation such that we were all on the same page for the big picture.

## Known Problems

We currently are not aware of any issues with our compiler.

## Comments

NA

